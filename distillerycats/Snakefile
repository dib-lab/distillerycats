import pandas as pd
import feather
from sourmash import signature
import glob
import os
from collections import Counter

metadata_file = config["metadata_file"]
m = pd.read_csv(metadata_file, header = 0)
SAMPLES = m['sample'].unique().tolist()
STUDY = m['study'].unique().tolist()

# basic snakemake reporting (customize later!)
onstart:
    print("------------------------------")
    print("Starting distillerycats workflow!")
    print("------------------------------")


onsuccess:
    print("\n--- Workflow executed successfully! ---\n")

onerror:
    print("Alas!\n")

rule all:
    input:
        "outputs/{alpha}-{ksize}/vita_rf/vita_vars_merged.sig",
        "outputs/{alpha}-{ksize}/comp/var_plt_all_filt_cosine.pdf",
        "outputs/{alpha}-{ksize}/comp/var_plt_all_filt_jaccard.pdf",
        #"outputs/{alpha}-{ksize}/gather_matches_lca/lca_summarize.csv",
        expand('outputs/{alpha}-{ksize}/optimal_rf/{study}_validation_acc.csv', study = STUDY)

# print out the configuration
rule showconf:
    run:
        import yaml
        print('# full aggregated configuration:')
        print(yaml.dump(config).strip())
        print('# END')

# check config files only
rule check:
    run:
        pass

########################################
## PREPROCESSING
########################################

rule fastp:
    input: 
        r1 = "inputs/raw/{sample}_R1.fq.gz",
        r2 = "inputs/raw/{sample}_R2.fq.gz"
    output: 
        r1 = 'outputs/fastp/{sample}_R1.fastp.fq.gz',
        r2 = 'outputs/fastp/{sample}_R2.fastp.fq.gz',
        json = 'outputs/fastp/{sample}.json'
    conda: 'envs/fastp.yml'
    shell:'''
    fastp -i {input.r1} -I {input.r2} -o {output.r1} -O {output.r2} -q 4 -j {output.json} -l 31 -c
    '''

rule download_human_db:
    output: "inputs/host/hg19_main_mask_ribo_animal_allplant_allfungus.fa.gz"
    shell:'''
    wget -O {output} https://osf.io/84d59/download
    '''

rule remove_host:
# http://seqanswers.com/forums/archive/index.php/t-42552.html
# https://drive.google.com/file/d/0B3llHR93L14wd0pSSnFULUlhcUk/edit?usp=sharing
    output:
        r1 = 'outputs/bbduk/{sample}_R1.nohost.fq.gz',
        r2 = 'outputs/bbduk/{sample}_R2.nohost.fq.gz',
        human_r1='outputs/bbduk/{sample}_R1.human.fq.gz',
        human_r2='outputs/bbduk/{sample}_R2.human.fq.gz'
    input: 
        r1 = 'outputs/fastp/{sample}_R1.fastp.fq.gz',
        r2 = 'outputs/fastp/{sample}_R2.fastp.fq.gz',
        human='inputs/host/hg19_main_mask_ribo_animal_allplant_allfungus.fa.gz'
    conda: 'envs/bbmap.yml'
    shell:'''
    bbduk.sh -Xmx64g t=3 in={input.r1} in2={input.r2} out={output.r1} out2={output.r2} outm={output.human_r1} outm2={output.human_r2} k=31 ref={input.human}
    '''

rule kmer_trim_reads:
    input: 
        'outputs/bbduk/{sample}_R1.nohost.fq.gz',
        'outputs/bbduk/{sample}_R2.nohost.fq.gz'
    output: "outputs/abundtrim/{sample}.abundtrim.fq.gz"
    conda: 'envs/sourmash.yml'
    shell:'''
    interleave-reads.py {input} | trim-low-abund.py --gzip -C 3 -Z 18 -M 60e9 -V - -o {output}
    '''

rule sourmash_sketch_nucleotide_input:
    input: "outputs/abundtrim/{sample}.abundtrim.fq.gz"
    output: "outputs/sigs/{sample}.sig"
    params:
        nucl_sketch="outputs/sigs/{sample}.nucleotide.sig"),
        prot_sketch="outputs/sigs/{sample}.translate.sig"),
        #signame = lambda w: accession2signame[w.accession],
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: attempt *1000,
        runtime=1200,
    log: "logs/sourmash/{sample}_sketch.log"
    conda: "envs/sourmash-dev.yml"
    shell:
        """
        sourmash sketch dna -p k=21,31,51 scaled=2000,abund \
                            -o {params.nucl_sketch} \
                            --name {wildcards.sample} {input}  2> {log}

        sourmash sketch translate -p protein k=7,8,9,10,11 scaled=100,abund \
                                  -p dayhoff k=15,17,19 scaled=100,abund \
                                  -p hp k=33,35 scaled=100,abund \
                                  -o {params.prot_sketch} \
                                  --name {wildcards.sample} {input}  2>> {log}

        sourmash sig cat {params.nucl_sketch} {params.prot_sketch} -o {output} 2>> {log}
        rm {params.nucl_sketch}
        rm {params.prot_sketch}
        """

########################################
## Filtering and formatting signatures
########################################


get_alpha_cmd = {"protein": "--protein --no-dna",
                 "dayhoff": "--dayhoff --no-dna --no-protein",
                 "hp": "--hp --no-dna --no-protein",
                 "dna": "--dna"}

rule drop_unique_hashes:
    input: expand("outputs/sigs/{sample}.sig", sample = SAMPLES)
    output: "outputs/{alpha}-{ksize}/filt_sig_hashes/greater_than_one_count_hashes.sig"
    conda: 'envs/sourmash.yml'
    shell:
        """
        python -m distillerycats.drop_unique_hashes.py --alphabet {wildcards.alpha} \
                                             --ksize {wildcards.ksize} \
                                             --minimum-count 2 \
                                             --output {output}
        """

rule filter_signatures_to_greater_than_1_hashes:
    input:
        filt_sig = "outputs/{alpha}-{ksize}/filt_sig_hashes/greater_than_one_count_hashes.sig",
        sig = "outputs/sigs/{sample}.sig"
    output: "outputs/{alpha}-{ksize}/filt_sigs/{sample}_filt.sig"
    params:
        alpha_cmd = lambda: get_alpha_cmd[w.alpha]
    conda: 'envs/sourmash.yml'
    shell:
        '''
        sourmash sig intersect -o {output} -A {input.sigs} \
                               -k {wildcards.ksize} {params.alpha_cmd} \
                               {input.sig} {input.filt_sig}
        '''

rule name_filtered_sigs:
    input: "outputs/{alpha}-{ksize}/filt_sigs/{sample}_filt.sig"
    output: "outputs/{alpha}-{ksize}/filt_sigs_named/{sample}_filt_named.sig"
    conda: 'envs/sourmash.yml'
    shell:'''
    sourmash sig rename -o {output} -k 31 {input} {wildcards.sample}_filt
    '''

rule describe_filtered_sigs:
    input: expand("outputs/{{alpha}}-{{ksize}}/filt_sigs_named/{sample}_filt_named.sig", sample = SAMPLES)
    output: "outputs/{alpha}-{ksize}/filt_sigs_named/sig_describe_filt_named_sig.csv"
    conda: 'envs/sourmash.yml'
    shell:'''
    sourmash signature describe --csv {output} {input}
    '''

rule convert_greater_than_1_signatures_to_csv:
    input: "outputs/{alpha}-{ksize}/filt_sigs_named/{sample}_filt_named.sig"
    output: "outputs/{alpha}-{ksize}/filt_sigs_named_csv/{sample}_filt_named.csv"
    conda: 'envs/sourmash.yml'
    shell:'''
    python -m distillerycats.sig_to_csv {input} {output}
    '''

rule make_hash_abund_table_long_normalized:
    input: 
        expand("outputs/{{alpha}}-{{ksize}}/filt_sigs_named_csv/{sample}_filt_named.csv", sample = SAMPLES)
    output: csv = "outputs/{alpha}-{ksize}/hash_tables/normalized_abund_hashes_long.csv"
    conda: 'envs/r.yml'
    script: "normalized_hash_abund_long.R"

rule make_hash_abund_table_wide:
    input: "outputs/{alpha}-{ksize}/hash_tables/normalized_abund_hashes_long.csv"
    output: "outputs/{alpha}-{ksize}/hash_tables/normalized_abund_hashes_wide.feather"
    run:
        import pandas as pd
        import feather
        
        ibd = pd.read_csv(str(input), dtype = {"minhash" : "int64", "abund" : "float64", "sample" : "object"})
        ibd_wide=ibd.pivot(index='sample', columns='minhash', values='abund')
        ibd_wide = ibd_wide.fillna(0)
        ibd_wide['sample'] = ibd_wide.index
        ibd_wide = ibd_wide.reset_index(drop=True)
        ibd_wide.columns = ibd_wide.columns.astype(str)
        ibd_wide.to_feather(str(output)) 

########################################
## Random forests & optimization
########################################

rule install_pomona:
    input: "outputs/{alpha}-{ksize}/hash_tables/normalized_abund_hashes_wide.feather"
    output:
        pomona = "outputs/vita_rf/pomona_install.txt"
    conda: 'envs/rf.yml'
    script: "install_pomona.R"

rule vita_var_sel_rf:
    input:
        info = metadata_file, 
        feather = "outputs/{alpha}-{ksize}/hash_tables/normalized_abund_hashes_wide.feather",
        pomona = "outputs/vita_rf/pomona_install.txt"
    output:
        vita_rf = "outputs/{alpha}-{ksize}/vita_rf/{study}_vita_rf.RDS",
        vita_vars = "outputs/{alpha}-{ksize}/vita_rf/{study}_vita_vars.txt",
        var_filt = "outputs/{alpha}-{ksize}/vita_rf/{study}_var_filt.csv"
    params: 
        threads = 4,
        validation_study = "{study}"
    conda: 'envs/rf.yml'
    script: "vita_rf.R"

rule loo_validation:
    input: 
        var_filt = 'outputs/{alpha}-{ksize}/vita_rf/{study}_var_filt.csv',
        info = metadata_file,
        eval_model = 'function_evaluate_model.R',
        ggconfusion = 'ggplotConfusionMatrix.R'
    output: 
        recommended_pars = 'outputs/{alpha}-{ksize}/optimal_rf/{study}_rec_pars.tsv',
        optimal_rf = 'outputs/{alpha}-{ksize}/optimal_rf/{study}_optimal_rf.RDS',
        training_accuracy = 'outputs/{alpha}-{ksize}/optimal_rf/{study}_training_acc.csv',
        training_confusion = 'outputs/{alpha}-{ksize}/optimal_rf/{study}_training_confusion.pdf',
        validation_accuracy = 'outputs/{alpha}-{ksize}/optimal_rf/{study}_validation_acc.csv',
        validation_confusion = 'outputs/{alpha}-{ksize}/optimal_rf/{study}_validation_confusion.pdf'
    params:
        threads = 5,
        validation_study = "{study}"
    conda: 'envs/tuneranger.yml'
    script: "tune_rf.R"


############################################
## Predictive hash characterization - gather
############################################

rule convert_vita_vars_to_sig:
    input: "outputs/{alpha}-{ksize}/vita_rf/{study}_vita_vars.txt"
    output: "outputs/{alpha}-{ksize}/vita_rf/{study}_vita_vars.sig"
    conda: "envs/sourmash.yml"
    shell:'''
    python -m distillerycats.hashvals_to_signature -o {output} -k 31 --scaled 2000 --name vita_vars --filename {input} {input}
    '''

# CHECK: this db contains all of previous dbs?
rule download_gather_genbank_dna:
    output: "gather_databases/genbank-k31.sbt.zip"
    params:
        dl_link="https://osf.io/jgu93/download"
    shell:
        """
        wget -O {output} {params.dl_link}
        """

# ADD PROTEIN DBS!

## MODIFY THIS TO MAP TO DB w/ correct alpha-ksize.
rule gather_vita_vars_all:
    input:
        sig="outputs/{alpha}-{ksize}/vita_rf/{study}_vita_vars.sig",
        db="inputs/gather_databases/genbank-k31.sbt.zip",
    output: 
        csv="outputs/{alpha}-{ksize}/gather/{study}_vita_vars_all.csv",
        matches="outputs/{alpha}-{ksize}/gather/{study}_vita_vars_all.matches",
        un="outputs/gather/{alpha}-{ksize}/{study}_vita_vars_all.un"
    conda: 'envs/sourmash.yml'
    shell:'''
    sourmash gather -o {output.csv} --save-matches {output.matches} --output-unassigned {output.un} --scaled 2000 -k 31 {input.sig} {input.db1} {input.db4} {input.db3} {input.db2}
    '''

rule merge_vita_vars_sig_all:
    input: expand("outputs/{{alpha}}-{{ksize}}/vita_rf/{study}_vita_vars.sig", study = STUDY)
    output: "outputs/{alpha}-{ksize}/vita_rf/vita_vars_merged.sig"
    conda: "envs/sourmash.yml"
    shell:'''
    sourmash sig merge -o {output} {input}
    '''

rule merge_vita_vars_matching_sigs_all:
    input: expand("outputs/{{alpha}}-{{ksize}}/gather/{study}_vita_vars_all.matches", study = STUDY)
    output: "outputs/{alpha}-{ksize}/vita_rf/vita_vars_matches_merged.sig"
    conda: "envs/sourmash.yml"
    shell:'''
    sourmash sig merge -o {output} {input}
    '''

rule combine_gather_vita_vars_all:
    input: expand("outputs/{{alpha}}-{{ksize}}/gather/{study}_vita_vars_all.csv", study = STUDY)
    output: "outputs/{alpha}-{ksize}/gather/vita_vars_all.csv"
    run:
        import pandas as pd
        
        li = []
        for filename in input:
            li.append(df)

        frame = pd.concat(li, axis=0, ignore_index=True)
        frame.to_csv(str(output))


rule create_hash_genome_map_gather_vita_vars_all:
    input:
        matches = "outputs/{alpha}-{ksize}/vita_rf/vita_vars_matches_merged.sig",
        vita_vars = "outputs/{alpha}-{ksize}/vita_rf/vita_vars_merged.sig"
    output: 
        hashmap = "outputs/{alpha}-{ksize}/gather_matches_hash_map/hash_to_genome_map_gather_all.csv",
        namemap = "outputs/{alpha}-{ksize}/gather_matches_hash_map/genome_name_to_filename_map_gather_all.csv"
    run:
        from sourmash import signature
        import pandas as pd
        
        # read in all genome signatures that had gather 
        # matches for the var imp hashes create a dictionary, 
        # where the key is the genome and the values are the minhashes.
        # also generate a list of all minhashes from all genomes. 
        sigfp = open(input.matches, "rt")
        siglist = list(signature.load_signatures(sigfp))
        genome_dict = {}
        all_mins = []
        sig_names = []
        sig_filenames = []
        for num, sig in enumerate(siglist):
            loaded_sig = siglist[num]
            mins = loaded_sig.minhash.get_mins() # Get the minhashes
            genome_dict[sig.name()] = mins
            all_mins += mins
            sig_names += sig.name()
            sig_filenames += sig.filename

        # read in vita variables
        sigfp = open(str(input.vita_vars), 'rt')
        vita_vars = sig = signature.load_one_signature(sigfp)
        vita_vars = vita_vars.minhash.get_mins() 

        # define a function where if a hash is a value, 
        # return all key for which it is a value
        def get_all_keys_if_value(dictionary, hash_query):
            genomes = list()
            for genome, v in dictionary.items():
                if hash_query in v:
                    genomes.append(genome)
            return genomes

        # create a dictionary where each vita_vars hash is a key, 
        # and values are the genome signatures in which that hash
        # is contained
        vita_hash_dict = {}
        for hashy in vita_vars:
            keys = get_all_keys_if_value(genome_dict, hashy)
            vita_hash_dict[hashy] = keys

        # transform this dictionary into a dataframe and format the info nicely
        df = pd.DataFrame(list(vita_hash_dict.values()), index = vita_hash_dict.keys())
        df = df.reset_index()
        df = pd.melt(df, id_vars=['index'], var_name= "drop", value_name='genome')
        # remove tmp col drop
        df = df.drop('drop', 1)
        # drop duplicate rows in the df
        df = df.drop_duplicates()
        # write the dataframe to csv
        df.to_csv(str(output.hashmap), index = False) 
        
        # generate a sig.name() : sig.filename() map, which will be helpful for
        # download the actual genomes later. 
        name_dict = {'sig_name': sig_names, 'sig_filename': sig_filenames} 
        name_df = pd.DataFrame(data = name_dict)
        name_df.to_csv(str(output.namemap), index = False)       


#rule download_sourmash_lca_db:
#    output: "inputs/gather_databases/gtdb-release89-k31.lca.json.gz"
#    shell:'''
#    wget -O {output} https://osf.io/gs29b/download
#    '''

#rule sourmash_lca_summarize_vita_vars_all_sig_matches:
#    input:
#        db = "inputs/gather_databases/gtdb-release89-k31.lca.json.gz",
#        matches = "outputs/{alpha}-{ksize}/vita_rf/vita_vars_matches_merged.sig",
#    output: "outputs/gather_matches_lca/lca_summarize.csv"
#    conda: "envs/sourmash.yml"
#    shell:'''
#    sourmash lca summarize --singleton --db {input.db} --query {input.matches} -o {output}
#    '''

###################################################
# Predictive hash characterization -- shared hashes
###################################################

########################################
## PCoA
########################################

rule compare_signatures_cosine:
    input: 
        expand("outputs/{{alpha}}-{{ksize}}/filt_sigs_named/{sample}_filt_named.sig", sample = SAMPLES),
    output: "outputs/{alpha}-{ksize}/comp/all_filt_comp_cosine.csv"
    conda: "envs/sourmash.yml"
    shell:'''
    sourmash compare -k 31 -p 8 --csv {output} {input}
    '''

rule compare_signatures_jaccard:
    input: 
        expand("outputs/{{alpha}}-{{ksize}}/filt_sigs_named/{sample}_filt_named.sig", sample = SAMPLES),
    output: "outputs/{alpha}-{ksize}/comp/all_filt_comp_jaccard.csv"
    conda: "envs/sourmash.yml"
    shell:'''
    sourmash compare --ignore-abundance -k 31 -p 8 --csv {output} {input}
    '''

rule permanova_jaccard:
    input: 
        comp = "outputs/{alpha}-{ksize}/comp/all_filt_comp_jaccard.csv",
        info = metadata_file,
        sig_info = "outputs/{alpha}-{ksize}/filt_sigs_named/sig_describe_filt_named_sig.csv"
    output: 
        perm = "outputs/{alpha}-{ksize}/comp/all_filt_permanova_jaccard.csv"
    conda: "envs/vegan.yml"
    script: "run_permanova.R"

rule permanova_cosine:
    input: 
        comp = "outputs/{alpha}-{ksize}/comp/all_filt_comp_cosine.csv",
        info = metadata_file,
        sig_info = "outputs/{alpha}-{ksize}/filt_sigs_named/sig_describe_filt_named_sig.csv"
    output: 
        perm = "outputs/{alpha}-{ksize}/comp/all_filt_permanova_cosine.csv"
    conda: "envs/vegan.yml"
    script: "run_permanova.R"

rule plot_comp_jaccard:
    input:
        comp = "outputs/{alpha}-{ksize}/comp/all_filt_comp_jaccard.csv",
        info = metadata_file
    output: 
        study = "outputs/{alpha}-{ksize}/comp/study_plt_all_filt_jaccard.pdf",
        var = "outputs/{alpha}-{ksize}/comp/var_plt_all_filt_jaccard.pdf"
    conda: "envs/ggplot.yml"
    script: "plot_comp.R"

rule plot_comp_cosine:
    input:
        comp = "outputs/{alpha}-{ksize}/comp/all_filt_comp_cosine.csv",
        info = metadata_file
    output: 
        study = "outputs/{alpha}-{ksize}/comp/study_plt_all_filt_cosine.pdf",
        var = "outputs/{alpha}-{ksize}/comp/var_plt_all_filt_cosine.pdf"
    conda: "envs/ggplot.yml"
    script: "plot_comp.R"


